count_data <- length(org_labels)
count_data
# разделим исходные данные на тестовую и обучающую выборки
# число новостей обучающей выборки (70% от исходных данных)
count_train = round(count_data*0.7)
count_train
# для применения методов классификации создаем контейнер
container <- create_container(
dtm,
labels = org_labels,
trainSize = 1:count_train,
testSize = (count_train+1):count_data,
virgin = FALSE
)
container
# проверим репрезентативность выборки
round(prop.table(table(frame[2]))*100, digits = 1)
train_data = frame[1:count_train,]
round(prop.table(table(train_data[2]))*100, digits = 1)
# обучение моделей
# Support vector machine
svm_model <- train_model(container, "SVM")
# Random forest
tree_model <- train_model(container, "TREE")
# Maximal entropy
maxent_model <- train_model(container, "MAXENT")
# запускаем классификацию
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
# создаем общую матрицу меток
labels_out <- data.frame(
correct_label = org_labels[(count_train+1):count_data],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
# SVM performance
table(labels_out[,1] == labels_out[,2])
# Random forest performance
table(labels_out[,1] == labels_out[,3])
# Maximum entropy performance
table(labels_out[,1] == labels_out[,4])
dtm <- removeSparseTerms(dtm, 0.9)
dtm
# список терминов
dtm$dimnames$Terms
# обучение моделей
# Support vector machine
svm_model <- train_model(container, "SVM")
# запускаем классификацию
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
# создаем общую матрицу меток
labels_out <- data.frame(
correct_label = org_labels[(count_train+1):count_data],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
# SVM performance
table(labels_out[,1] == labels_out[,2])
# Random forest performance
table(labels_out[,1] == labels_out[,3])
# Maximum entropy performance
table(labels_out[,1] == labels_out[,4])
dtm <- removeSparseTerms(dtm, 0.95)
dtm
for(i in c(2, 11, 12, 9, 13))
{
# читаем новости текущей категории
news_tmp <- readLines(paste0("newstext", i, ".txt"))
categ_tmp = rep(name_category[i], length(news_tmp))
news = append(news, news_tmp)
categ = append(categ, categ_tmp)
}
# сохраним информацию о новостях в data.frame
news_frame = data.frame("news" = news, "category" = categ)
# общее число новостей
nrow(news_frame)
# перемешаем исходные данные
set.seed(0)
frame = news_frame[order(runif(nrow(news_frame))),]
# создаем пустой объект корпус
news=list();
news_corpus = VCorpus(VectorSource(news))
# добавляем в корпус информацию о новостях всех тематик:
for(i in 1:nrow(frame))
{
# добавляем в корпус информацию
tmp_corpus <- VCorpus(VectorSource(frame[i,]$news))
news_corpus <- c(news_corpus, tmp_corpus)
# добавляем метаинформацию
meta(news_corpus[[i]], "category") <- frame[i,]$category
meta(news_corpus[[i]], "language") <- "ru"
}
news_corpus
meta(news_corpus[[1]])
# список тематик новостей:
meta_data<- data.frame()
# прочитаем новости с диска в переменную news,
# а категории, которым принадлежат эти новости, в переменную categ
news = character(0);
categ = character(0);
for(i in c(2, 11, 12, 9, 13))
{
# читаем новости текущей категории
news_tmp <- readLines(paste0("newstext", i, ".txt"))
categ_tmp = rep(name_category[i], length(news_tmp))
news = append(news, news_tmp)
categ = append(categ, categ_tmp)
}
# сохраним информацию о новостях в data.frame
news_frame = data.frame("news" = news, "category" = categ)
# общее число новостей
nrow(news_frame)
# перемешаем исходные данные
set.seed(0)
frame = news_frame[order(runif(nrow(news_frame))),]
# создаем пустой объект корпус
news=list();
news_corpus = VCorpus(VectorSource(news))
# добавляем в корпус информацию о новостях всех тематик:
for(i in 1:nrow(frame))
{
# добавляем в корпус информацию
tmp_corpus <- VCorpus(VectorSource(frame[i,]$news))
news_corpus <- c(news_corpus, tmp_corpus)
# добавляем метаинформацию
meta(news_corpus[[i]], "category") <- frame[i,]$category
meta(news_corpus[[i]], "language") <- "ru"
}
news_corpus
meta(news_corpus[[1]])
# список тематик новостей:
meta_data<- data.frame()
for (i in 1:NROW(news_corpus))
{
meta_data [i, "category"] <- meta(news_corpus[[i]], "category")
meta_data [i, "num"] <- i
}
table(as.character(meta_data[, "category"]))
# удалим числа
news_corpus <- tm_map(news_corpus, content_transformer(removeNumbers))
# заменим на пробелы знаки пунктуации
news_corpus <- tm_map(news_corpus,
content_transformer(str_replace_all),
pattern = "[[:punct:]]", replacement = " ")
# преобразуем к нижнему регистру
news_corpus <- tm_map(news_corpus, content_transformer(tolower))
# удаляем стоп-слова
news_corpus <- tm_map(news_corpus, content_transformer(removeWords), words =  stopwords("ru"))
# проводим стемминг
for(i in 1:NROW(news_corpus))
news_corpus[[i]]$content <- stemDocument(news_corpus[[i]]$content, language = "russian")
news_corpus <- tm_map(news_corpus, content_transformer(removeWords), words =  'котор')
# создаем матрицу терминов-документов
tdm <- TermDocumentMatrix(news_corpus)
tdm
# создаем матрицу документов-терминов
dtm <- DocumentTermMatrix(news_corpus)
dtm
dtm <- removeSparseTerms(dtm, 0.97)
dtm
dtm <- removeSparseTerms(dtm, 0.97)
dtm
# создаем матрицу документов-терминов
dtm <- DocumentTermMatrix(news_corpus)
dtm
# массив меток – тематики новостей
org_labels<-meta_data[, "category"]
count_data <- length(org_labels)
count_data
# разделим исходные данные на тестовую и обучающую выборки
# число новостей обучающей выборки (70% от исходных данных)
count_train = round(count_data*0.7)
# для применения методов классификации создаем контейнер
container <- create_container(
dtm,
labels = org_labels,
trainSize = 1:count_train,
testSize = (count_train+1):count_data,
virgin = FALSE
)
# проверим репрезентативность выборки
round(prop.table(table(frame[2]))*100, digits = 1)
train_data = frame[1:count_train,]
round(prop.table(table(train_data[2]))*100, digits = 1)
# обучение моделей
# Support vector machine
svm_model <- train_model(container, "SVM")
# Random forest
tree_model <- train_model(container, "TREE")
# Maximal entropy
maxent_model <- train_model(container, "MAXENT")
# запускаем классификацию
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
# создаем общую матрицу меток
labels_out <- data.frame(
correct_label = org_labels[(count_train+1):count_data],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
# SVM performance
table(labels_out[,1] == labels_out[,2])
# Random forest performance
table(labels_out[,1] == labels_out[,3])
# Maximum entropy performance
table(labels_out[,1] == labels_out[,4])
# назначим рабочий директорий
setwd("C:/Users/admin.WIN-OSD1NEIENB0/Documents/Rprojects/task4")
write('12330', paste0('Categ/acaterg', 2, '.txt')
)
write('12330', paste0('Categ/acaterg', 2, '.txt'))
# назначим рабочий директорий
setwd("C:/Users/admin.WIN-OSD1NEIENB0/Documents/Rprojects/task4/tass")
write('12330', paste0('Categ/acaterg', 2, '.txt'))
# назначим рабочий директорий
setwd("C:/Users/admin.WIN-OSD1NEIENB0/Documents/Rprojects/task4/tass")
# назначим рабочий директорий
setwd("C:/Users/admin.WIN-OSD1NEIENB0/Documents/Rprojects/task4/rambler")
library(RCurl) # основной пакет для веб-скрепинга
library(XML) # пакет для  работы с XML, HTML, Xpath
library(stringr) # пакет для работы с регулярными выражениями
# новости, сми
main = "https://rambler.ru/"
# создаем объект для работы по протоколу SSL
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
url = getURL(main, cainfo = signatures, encoding="UTF-8")
write(url, 'ramblerurl.txt')
# новости, сми
main = "https://news.rambler.ru/"
# создаем объект для работы по протоколу SSL
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
url = getURL(main, cainfo = signatures, encoding="UTF-8")
write(url, 'ramblerurl.txt')
# создаем html-объект
webpage <- htmlParse(url, encoding="UTF-8")
webpage
# вытащим информацию о тематике новостей
category <- xpathSApply(webpage, "//a[@class='level__title level__title--news']", xmlGetAttr, "href");
category = category[-(15:19)]
# вытащим информацию о тематике новостей
category <- xpathSApply(webpage, "//a[@class='level__title level__title--news']", xmlGetAttr, "href");
category
category = category[-2]
main1 = substr(main, 1, nchar(main)-1)
i = 1
category[i] = paste0(main1, category[i])
categ_url=getURL(category[i], cainfo = signatures, encoding="UTF-8")
# загружаем все страницы на диск:
write(categ_url, paste0('Categ/categ', i, '.txt'))
catpage <- htmlParse(categ_url, encoding="UTF-8")
#ссылки на новости;
ref = character();
# убираем переносы строк:
tmp <- str_c(categ_url, collapse = "")
# строим HTML-объект:
tmp <- htmlParse(tmp, encoding="UTF-8")
# получаем ссылки на все новости текущей тематики
news_page <- xpathSApply(tmp, "//div[@class='top-topic__news-item']/a", xmlGetAttr, "href")
news_page
ref = append(ref, paste0(main1,news_page))
for(j in 1:length(ref))
print(j)
j =1
news1_url= getURL(ref[j], cainfo = signatures, encoding="UTF-8");
write(news1_url, paste0('news1_, j, '.txt'))
news1_page <- htmlParse(news1_url, encoding="UTF-8")
news1_text <- xpathSApply(news1_page, "//div[@class='text-block']/p", xmlValue)
# удаляем из текста знаки переноса строки, табуляции, лишние пробелы:
news1_text = str_replace_all(news1_text, "\n", " ");
news1_text = str_replace_all(news1_text, "\t", " ");
news1_text = str_replace_all(news1_text, "[ ]{2,}", "");
# все строки новости записываем в одну переменную full_text:
full_text = "";
for(k in 1:length(news1_text))
{
full_text = str_c(full_text, paste0(" ", news1_text[k]));
}
# записываем текст новости на диск:
write(full_text, paste0("Newstext/newstext", i, ".txt"), append = TRUE)
}
}
# пакет Text mining
library(tm)
# получим названия тематик новостей
#name_category <- xpathSApply(webpage, "//div[@class='menu-menyu-1-container']/ul[@class='menu']/li[@class='menu-item menu-item-has-children']/ul[@class='sub-menu']/li/a", xmlValue);
name_category = category
# прочитаем новости с диска в переменную news,
# а категории, которым принадлежат эти новости, в переменную categ
news = character(0);
categ = character(0);
# для классификации выберем новости о политике, спорте и культуре (обществе 9, науке 13)
for(i in c(2, 11, 12)) #9, 13
{
# читаем новости текущей категории
news_tmp <- readLines(paste0("Newstext/newstext", i, ".txt"))
categ_tmp = rep(name_category[i], length(news_tmp))
news = append(news, news_tmp)
categ = append(categ, categ_tmp)
}
# сохраним информацию о новостях в data.frame
news_frame = data.frame("news" = news, "category" = categ)
# общее число новостей
nrow(news_frame)
# перемешаем исходные данные
set.seed(0)
frame = news_frame[order(runif(nrow(news_frame))),]
# создаем пустой объект корпус
news=list();
news_corpus = VCorpus(VectorSource(news))
# добавляем в корпус информацию о новостях всех тематик:
for(i in 1:nrow(frame))
{
# добавляем в корпус информацию
tmp_corpus <- VCorpus(VectorSource(frame[i,]$news))
news_corpus <- c(news_corpus, tmp_corpus)
# добавляем метаинформацию
meta(news_corpus[[i]], "category") <- frame[i,]$category
meta(news_corpus[[i]], "language") <- "ru"
}
news_corpus
meta(news_corpus[[1]])
# список тематик новостей:
meta_data<- data.frame()
for (i in 1:NROW(news_corpus))
{
meta_data [i, "category"] <- meta(news_corpus[[i]], "category")
meta_data [i, "num"] <- i
}
table(as.character(meta_data[, "category"]))
news_corpus[[11]]$content
# удалим числа
news_corpus <- tm_map(news_corpus, content_transformer(removeNumbers))
news_corpus[[11]]$content
# заменим на пробелы знаки пунктуации
news_corpus <- tm_map(news_corpus,
content_transformer(str_replace_all),
pattern = "[[:punct:]]", replacement = " ")
news_corpus[[11]]$content
# преобразуем к нижнему регистру
news_corpus <- tm_map(news_corpus, content_transformer(tolower))
news_corpus[[11]]$content
# удаляем стоп-слова
news_corpus <- tm_map(news_corpus, content_transformer(removeWords), words =  stopwords("ru"))
news_corpus[[11]]$content
# проводим стемминг
for(i in 1:NROW(news_corpus))
news_corpus[[i]]$content <- stemDocument(news_corpus[[i]]$content, language = "russian")
news_corpus <- tm_map(news_corpus, content_transformer(removeWords), words =  'котор')
news_corpus[[11]]$content
# создадим облако слов
library(wordcloud)
wordcloud(news_corpus, random.order=F, max.words=40)
# создаем матрицу терминов-документов
tdm <- TermDocumentMatrix(news_corpus)
tdm
# создаем матрицу документов-терминов
dtm <- DocumentTermMatrix(news_corpus)
dtm
dtm <- removeSparseTerms(dtm, 0.97)
dtm
# список терминов
dtm$dimnames$Terms
# дополнительный пакет для работы с текстами
library(RTextTools)
# массив меток – тематики новостей
org_labels<-meta_data[, "category"]
count_data <- length(org_labels)
count_data
# разделим исходные данные на тестовую и обучающую выборки
# число новостей обучающей выборки (70% от исходных данных)
count_train = round(count_data*0.7)
count_train
# для применения методов классификации создаем контейнер
container <- create_container(
dtm,
labels = org_labels,
trainSize = 1:count_train,
testSize = (count_train+1):count_data,
virgin = FALSE
)
container
# проверим репрезентативность выборки
round(prop.table(table(frame[2]))*100, digits = 1)
train_data = frame[1:count_train,]
round(prop.table(table(train_data[2]))*100, digits = 1)
# обучение моделей
# Support vector machine
svm_model <- train_model(container, "SVM")
# Random forest
tree_model <- train_model(container, "TREE")
# Maximal entropy
maxent_model <- train_model(container, "MAXENT")
# запускаем классификацию
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
# создаем общую матрицу меток
labels_out <- data.frame(
correct_label = org_labels[(count_train+1):count_data],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
# SVM performance
table(labels_out[,1] == labels_out[,2])
# Random forest performance
table(labels_out[,1] == labels_out[,3])
# Maximum entropy performance
table(labels_out[,1] == labels_out[,4])
# назначим рабочий директорий
setwd("C:/Users/admin.WIN-OSD1NEIENB0/Documents/Rprojects/task4/rambler")
library(RCurl) # основной пакет для веб-скрепинга
library(XML) # пакет для  работы с XML, HTML, Xpath
library(stringr) # пакет для работы с регулярными выражениями
# новости, сми
main = "https://news.rambler.ru/"
# создаем объект для работы по протоколу SSL
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
url = getURL(main, cainfo = signatures, encoding="UTF-8")
write(url, 'ramblerurl.txt')
# создаем html-объект
webpage <- htmlParse(url, encoding="UTF-8")
webpage
# вытащим информацию о тематике новостей
category <- xpathSApply(webpage, "//a[@class='level__title level__title--news']", xmlGetAttr, "href");
category = category[-2]
category
main1 = substr(main, 1, nchar(main)-1)
for(i in 1:length(category))
{
category[i] = paste0(main1, category[i])
categ_url=getURL(category[i], cainfo = signatures, encoding="UTF-8")
# загружаем все страницы на диск:
write(categ_url, paste0('Categ/categ', i, '.txt'))
catpage <- htmlParse(categ_url, encoding="UTF-8")
#catpage
#ссылки на новости;
ref = character();
# убираем переносы строк:
tmp <- str_c(categ_url, collapse = "")
# строим HTML-объект:
tmp <- htmlParse(tmp, encoding="UTF-8")
# получаем ссылки на все новости текущей тематики
news_page <- xpathSApply(tmp, "//div[@class='top-topic__news-item']/a", xmlGetAttr, "href")
news_page
ref = append(ref, paste0(main1,news_page))
# записываем на диск тексты всех новостей текущей тематики
for(j in 1:length(ref))
{
print(j)
news1_url= getURL(ref[j], cainfo = signatures, encoding="UTF-8");
write(news1_url, paste0('news1_', j, '.txt'))
news1_page <- htmlParse(news1_url, encoding="UTF-8")
news1_text <- xpathSApply(news1_page, "//div[@class='text-block']/p", xmlValue)
# удаляем из текста знаки переноса строки, табуляции, лишние пробелы:
news1_text = str_replace_all(news1_text, "\n", " ");
news1_text = str_replace_all(news1_text, "\t", " ");
news1_text = str_replace_all(news1_text, "[ ]{2,}", "");
# все строки новости записываем в одну переменную full_text:
full_text = "";
for(k in 1:length(news1_text))
{
full_text = str_c(full_text, paste0(" ", news1_text[k]));
}
# записываем текст новости на диск:
write(full_text, paste0("Newstext/newstext", i, ".txt"), append = TRUE)
}
}
# назначим рабочий директорий
setwd("C:/Users/admin.WIN-OSD1NEIENB0/Documents/Rprojects/task4/rambler")
library(RCurl) # основной пакет для веб-скрепинга
library(XML) # пакет для  работы с XML, HTML, Xpath
library(stringr) # пакет для работы с регулярными выражениями
# новости, сми
main = "https://news.rambler.ru/"
# создаем объект для работы по протоколу SSL
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
url = getURL(main, cainfo = signatures, encoding="UTF-8")
write(url, 'ramblerurl.txt')
# создаем html-объект
webpage <- htmlParse(url, encoding="UTF-8")
# вытащим информацию о тематике новостей
category <- xpathSApply(webpage, "//a[@class='level__title level__title--news']", xmlGetAttr, "href");
category = category[-2]
category
main1 = substr(main, 1, nchar(main)-1)
i = 1
category[i] = paste0(main1, category[i])
categ_url=getURL(category[i], cainfo = signatures, encoding="UTF-8")
# загружаем все страницы на диск:
write(categ_url, paste0('Categ/categ', i, '.txt'))
catpage <- htmlParse(categ_url, encoding="UTF-8")
#ссылки на новости;
ref = character();
# убираем переносы строк:
tmp <- str_c(categ_url, collapse = "")
# строим HTML-объект:
tmp <- htmlParse(tmp, encoding="UTF-8")
# получаем ссылки на все новости текущей тематики
news_page <- xpathSApply(tmp, "//div[@class='top-topic__news-item']/a", xmlGetAttr, "href")
ref = append(ref, paste0(main1,news_page))
j = 1
news1_url= getURL(ref[j], cainfo = signatures, encoding="UTF-8");
write(news1_url, paste0('News/news', i, '_', j, '.txt'))
ref[j]
