mileage = append(mileage, 0);
}
k = str_extract(str_info[index], "[(][:digit:]+");
k = as.numeric(str_replace(k, '[(]', ''));
power = append(power, k);
y = str_extract(str_info[index], "[MA]?T");
transmission = append(transmission, y);
z = str_extract(str_info[index], "[:digit:]+[.][:digit:]+");
engine_capacity = append(engine_capacity, as.numeric(z))
body_type = append(body_type, str_info[index+1])
drive_unit = append(drive_unit, str_info[index+2])
engine_type = append(engine_type, str_info[index+3])
}
#пробег, км
mileage
#мощность л.с.
power
#трансмиссия (механика/автомат)
transmission
#объем двигателя, л
engine_capacity
#тип кузова
body_type
#привод
drive_unit
#тип двигателя
engine_type
petdis = numeric(); #тип двигателя - бензин(0) / дизель (1)
for(i in 1:length(engine_type))
{
if(engine_type[i] == 'бензин')
{
petdis[i] = 0;
}
else
{
petdis[i] = 1;
}
}
petdis
bt = numeric(); #тип кузова - хетчбэк, внедорожник(0) / универсал, минивэн(1)
for(i in 1:length(body_type))
{
if(body_type[i] == 'хетчбэк' || body_type[i] == 'внедорожник')
{
bt[i] = 0;
}
else
{
bt[i] = 1;
}
}
bt
mileage = mileage / 1000;
price = price / 1000;
# сохраним преобразованные поля как data.frame
parse_data = data.frame("year" = year, "is_broken" = is_broken, "mileage" = mileage, "power" = power,
"engine_capacity" = engine_capacity, "bt" = bt, "petdis" = petdis,
"price" = price);
parse_data
# информация о данных
summary(parse_data)
# вычислим попарные коэффициенты корреляции между предикторами
r = cor(parse_data)
r
# построим регрессионную модель
mymodel = lm(price ~ year + is_broken + mileage + power + engine_capacity + bt + petdis, data = parse_data)
mymodel
summary(mymodel)
# построим регрессионную модель
mymodel = lm(price ~ year + is_broken + mileage, data = parse_data)
mymodel
summary(mymodel)
# данные для прогноза
predict_data = parse_data
predict_data = predict_data[-8]
predict_data
# спрогнозируем цену автомобилей по нашей модели
predictPrice = predict(mymodel, predict_data)
predictPrice = as.data.frame(predictPrice)
predictPrice
# истинная цена автомобилей
truePrice = parse_data$price
truePrice
# ошибка прогноза
error = abs(truePrice - predictPrice)*100/truePrice;
error= round(error, 3)
colnames(error) = "error"
max(error)
# таблица результата прогноза
tableResults = data.frame(predict_data, truePrice, predictPrice, error)
tableResults
# назначим рабочий директорий
setwd("C:/Users/admin.WIN-OSD1NEIENB0/Documents/Rprojects/task4")
library(RCurl) # основной пакет для веб-скрепинга
library(XML) # пакет для  работы с XML, HTML, Xpath
library(stringr) # пакет для работы с регулярными выражениями
# новости, сми
main = "https://tass.ru/"
# создаем объект для работы по протоколу SSL
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
url = getURL(main, cainfo = signatures, encoding="UTF-8")
write(url, 'tassurl.txt')
# создаем html-объект
webpage <- htmlParse(url, encoding="UTF-8")
# вытащим информацию о тематике новостей
category <- xpathSApply(webpage, "//div[@class='menu-sections-list__title-wrapper']/a[@class='menu-sections-list__title']", xmlGetAttr, "href");
category = category[-(15:19)]
category
main1 = substr(main, 1, nchar(main)-1)
for(i in 1:length(category))
{
category[i] = paste0(main1, category[i])
categ_url=getURL(category[i], cainfo = signatures, encoding="UTF-8")
# загружаем все страницы на диск:
write(categ_url, paste0('categ', i, '.txt'))
#Sys.sleep(1);
catpage <- htmlParse(categ_url, encoding="UTF-8")
#catpage
# получаем номер последней страницы:
#last_page_numb <- xpathSApply(catpage, "//a[@class='last']", xmlGetAttr, "href");
#last_page_numb = as.numeric(str_extract(last_page_numb, "[:digit:]+"));
#last_page_numb
# получим список страниц;
#list_of_pages = character();
#list_of_pages = append(list_of_pages, category[i]);
#if(length(last_page_numb) != 0)
#  list_of_pages = append(list_of_pages, str_c(category[i], paste0('page/', 2:last_page_numb, '/')));
#list_of_pages
# загружаем все страницы на диск:
#for(j in 1:length(list_of_pages)){
#  tmp <- getURL(list_of_pages[j], cainfo = signatures, encoding="UTF-8")
#  write(tmp, str_c(paste0("Category", i, "/"), j, ".html"))
#}
# загружаем все страницы на диск:
#tmp <- getURL(category[i], cainfo = signatures, encoding="UTF-8")
#write(tmp, paste0("Category", i, ".html"))
#ссылки на новости;
ref = character();
#if(length(last_page_numb) == 0)
#  last_page_numb = 1;
# проходим по всем загруженным страницам
# получаем ссылки на все новости текущей тематики и сохраняем их в переменную ref
#for(j in 1:last_page_numb)
#{
# читаем текущую страницу
# tmp <- readLines(paste0("Category", i, ".html"))
# убираем переносы строк:
tmp <- str_c(categ_url, collapse = "")
# строим HTML-объект:
tmp <- htmlParse(tmp, encoding="UTF-8")
# получаем ссылки на все новости текущей тематики
news_page <- xpathSApply(tmp, "//a[@class='news-preview news-preview_default']", xmlGetAttr, "href")
news_page
ref = append(ref, paste0(main1,news_page))
#}
# записываем на диск тексты всех новостей текущей тематики
for(j in 1:length(ref))
{
print(j)
news1_url= getURL(ref[j], cainfo = signatures, encoding="UTF-8");
#write(news1_url, 'news1.txt')
news1_page <- htmlParse(news1_url, encoding="UTF-8")
news1_text <- xpathSApply(news1_page, "//div[@class='text-block']/p", xmlValue)
#if(length(news1_text) != 0){
#news1_text <- xpathSApply(news1_page, "//div[@class='text-block']//p[following-sibling::style]", xmlValue)
#}else
#{
#  news1_text <- xpathSApply(news1_page, "//div[@class='text-block']", xmlValue);
#}
# удаляем из текста знаки переноса строки, табуляции, лишние пробелы:
news1_text = str_replace_all(news1_text, "\n", " ");
news1_text = str_replace_all(news1_text, "\t", " ");
news1_text = str_replace_all(news1_text, "[ ]{2,}", "");
# все строки новости записываем в одну переменную full_text:
full_text = "";
for(k in 1:length(news1_text))
{
full_text = str_c(full_text, paste0(" ", news1_text[k]));
}
# записываем текст новости на диск:
write(full_text, paste0("newstext", i, ".txt"), append = TRUE)
}
}
#ссылки на новости;
ref = character();
#for(j in 1:last_page_numb)
#{
# читаем текущую страницу
# tmp <- readLines(paste0("Category", i, ".html"))
# убираем переносы строк:
tmp <- str_c(categ_url, collapse = "")
# строим HTML-объект:
tmp <- htmlParse(tmp, encoding="UTF-8")
# получаем ссылки на все новости текущей тематики
news_page <- xpathSApply(tmp, "//a[@class='news-preview news-preview_default']", xmlGetAttr, "href")
news_page
ref = append(ref, paste0(main1,news_page))
for(j in 1:length(ref))
news1_url= getURL(ref[j], cainfo = signatures, encoding="UTF-8");
write(news1_url, 'news1.txt')
news1_page <- htmlParse(news1_url, encoding="UTF-8")
news1_text <- xpathSApply(news1_page, "//div[@class='text-block']/p", xmlValue)
# удаляем из текста знаки переноса строки, табуляции, лишние пробелы:
news1_text = str_replace_all(news1_text, "\n", " ");
news1_text = str_replace_all(news1_text, "\t", " ");
news1_text = str_replace_all(news1_text, "[ ]{2,}", "");
# пакет Text mining
library(tm)
# получим названия тематик новостей
#name_category <- xpathSApply(webpage, "//div[@class='menu-menyu-1-container']/ul[@class='menu']/li[@class='menu-item menu-item-has-children']/ul[@class='sub-menu']/li/a", xmlValue);
#name_category=name_category[-8]
#name_category=name_category[-7]
name_category = category
# прочитаем новости с диска в переменную news,
# а категории, которым принадлежат эти новости, в переменную categ
news = character(0);
categ = character(0);
for(i in c(2, 11, 12))
{
# читаем новости текущей категории
news_tmp <- readLines(paste0("newstext", i, ".txt"))
categ_tmp = rep(name_category[i], length(news_tmp))
news = append(news, news_tmp)
categ = append(categ, categ_tmp)
}
# сохраним информацию о новостях в data.frame
news_frame = data.frame("news" = news, "category" = categ)
# общее число новостей
nrow(news_frame)
View(news_frame)
# перемешаем исходные данные
set.seed(0)
frame = news_frame[order(runif(nrow(news_frame))),]
View(frame)
# создаем пустой объект корпус
news=list();
news_corpus = VCorpus(VectorSource(news))
# добавляем в корпус информацию о новостях всех тематик:
for(i in 1:nrow(frame))
{
# добавляем в корпус информацию
tmp_corpus <- VCorpus(VectorSource(frame[i,]$news))
news_corpus <- c(news_corpus, tmp_corpus)
# добавляем метаинформацию
meta(news_corpus[[i]], "category") <- frame[i,]$category
meta(news_corpus[[i]], "language") <- "ru"
}
news_corpus
meta(news_corpus[[1]])
# список тематик новостей:
meta_data<- data.frame()
for (i in 1:NROW(news_corpus))
{
meta_data [i, "category"] <- meta(news_corpus[[i]], "category")
meta_data [i, "num"] <- i
}
table(as.character(meta_data[, "category"]))
news_corpus[[11]]$content
# удалим числа
news_corpus <- tm_map(news_corpus, content_transformer(removeNumbers))
news_corpus[[11]]$content
# заменим на пробелы знаки пунктуации
news_corpus <- tm_map(news_corpus,
content_transformer(str_replace_all),
pattern = "[[:punct:]]", replacement = " ")
news_corpus[[11]]$content
# преобразуем к нижнему регистру
news_corpus <- tm_map(news_corpus, content_transformer(tolower))
news_corpus[[11]]$content
# удаляем стоп-слова
news_corpus <- tm_map(news_corpus, content_transformer(removeWords), words =  stopwords("ru"))
news_corpus[[11]]$content
# проводим стемминг
for(i in 1:NROW(news_corpus))
news_corpus[[i]]$content <- stemDocument(news_corpus[[i]]$content, language = "russian")
news_corpus <- tm_map(news_corpus, content_transformer(removeWords), words =  'котор')
news_corpus[[11]]$content
# создадим облако слов
library(wordcloud)
install.packages("worldcloud")
install.packages("wordcloud")
# создадим облако слов
library(wordcloud)
wordcloud(news_corpus, random.order=F, max.words=40)
# создаем матрицу терминов-документов
tdm <- TermDocumentMatrix(news_corpus)
tdm
# создаем матрицу документов-терминов
dtm <- DocumentTermMatrix(news_corpus)
dtm
dtm <- removeSparseTerms(dtm, 0.97)
dtm
# список терминов
dtm$dimnames$Terms
# дополнительный пакет для работы с текстами
library(RTextTools)
# массив меток – тематики новостей
org_labels<-meta_data[, "category"]
count_data <- length(org_labels)
count_data
# разделим исходные данные на тестовую и обучающую выборки
# число новостей обучающей выборки (70% от исходных данных)
count_train = round(count_data*0.7)
count_train
# для применения методов классификации создаем контейнер
container <- create_container(
dtm,
labels = org_labels,
trainSize = 1:count_train,
testSize = (count_train+1):count_data,
virgin = FALSE
)
container
# проверим репрезентативность выборки
round(prop.table(table(frame[2]))*100, digits = 1)
train_data = frame[1:count_train,]
round(prop.table(table(train_data[2]))*100, digits = 1)
# обучение моделей
# Support vector machine
svm_model <- train_model(container, "SVM")
# Random forest
tree_model <- train_model(container, "TREE")
# Maximal entropy
maxent_model <- train_model(container, "MAXENT")
# запускаем классификацию
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
# создаем общую матрицу меток
labels_out <- data.frame(
correct_label = org_labels[(count_train+1):count_data],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
# SVM performance
table(labels_out[,1] == labels_out[,2])
# Random forest performance
table(labels_out[,1] == labels_out[,3])
# Maximum entropy performance
table(labels_out[,1] == labels_out[,4])
dtm <- removeSparseTerms(dtm, 0.9)
dtm
# список терминов
dtm$dimnames$Terms
# обучение моделей
# Support vector machine
svm_model <- train_model(container, "SVM")
# запускаем классификацию
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
# создаем общую матрицу меток
labels_out <- data.frame(
correct_label = org_labels[(count_train+1):count_data],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
# SVM performance
table(labels_out[,1] == labels_out[,2])
# Random forest performance
table(labels_out[,1] == labels_out[,3])
# Maximum entropy performance
table(labels_out[,1] == labels_out[,4])
dtm <- removeSparseTerms(dtm, 0.95)
dtm
for(i in c(2, 11, 12, 9, 13))
{
# читаем новости текущей категории
news_tmp <- readLines(paste0("newstext", i, ".txt"))
categ_tmp = rep(name_category[i], length(news_tmp))
news = append(news, news_tmp)
categ = append(categ, categ_tmp)
}
# сохраним информацию о новостях в data.frame
news_frame = data.frame("news" = news, "category" = categ)
# общее число новостей
nrow(news_frame)
# перемешаем исходные данные
set.seed(0)
frame = news_frame[order(runif(nrow(news_frame))),]
# создаем пустой объект корпус
news=list();
news_corpus = VCorpus(VectorSource(news))
# добавляем в корпус информацию о новостях всех тематик:
for(i in 1:nrow(frame))
{
# добавляем в корпус информацию
tmp_corpus <- VCorpus(VectorSource(frame[i,]$news))
news_corpus <- c(news_corpus, tmp_corpus)
# добавляем метаинформацию
meta(news_corpus[[i]], "category") <- frame[i,]$category
meta(news_corpus[[i]], "language") <- "ru"
}
news_corpus
meta(news_corpus[[1]])
# список тематик новостей:
meta_data<- data.frame()
# прочитаем новости с диска в переменную news,
# а категории, которым принадлежат эти новости, в переменную categ
news = character(0);
categ = character(0);
for(i in c(2, 11, 12, 9, 13))
{
# читаем новости текущей категории
news_tmp <- readLines(paste0("newstext", i, ".txt"))
categ_tmp = rep(name_category[i], length(news_tmp))
news = append(news, news_tmp)
categ = append(categ, categ_tmp)
}
# сохраним информацию о новостях в data.frame
news_frame = data.frame("news" = news, "category" = categ)
# общее число новостей
nrow(news_frame)
# перемешаем исходные данные
set.seed(0)
frame = news_frame[order(runif(nrow(news_frame))),]
# создаем пустой объект корпус
news=list();
news_corpus = VCorpus(VectorSource(news))
# добавляем в корпус информацию о новостях всех тематик:
for(i in 1:nrow(frame))
{
# добавляем в корпус информацию
tmp_corpus <- VCorpus(VectorSource(frame[i,]$news))
news_corpus <- c(news_corpus, tmp_corpus)
# добавляем метаинформацию
meta(news_corpus[[i]], "category") <- frame[i,]$category
meta(news_corpus[[i]], "language") <- "ru"
}
news_corpus
meta(news_corpus[[1]])
# список тематик новостей:
meta_data<- data.frame()
for (i in 1:NROW(news_corpus))
{
meta_data [i, "category"] <- meta(news_corpus[[i]], "category")
meta_data [i, "num"] <- i
}
table(as.character(meta_data[, "category"]))
# удалим числа
news_corpus <- tm_map(news_corpus, content_transformer(removeNumbers))
# заменим на пробелы знаки пунктуации
news_corpus <- tm_map(news_corpus,
content_transformer(str_replace_all),
pattern = "[[:punct:]]", replacement = " ")
# преобразуем к нижнему регистру
news_corpus <- tm_map(news_corpus, content_transformer(tolower))
# удаляем стоп-слова
news_corpus <- tm_map(news_corpus, content_transformer(removeWords), words =  stopwords("ru"))
# проводим стемминг
for(i in 1:NROW(news_corpus))
news_corpus[[i]]$content <- stemDocument(news_corpus[[i]]$content, language = "russian")
news_corpus <- tm_map(news_corpus, content_transformer(removeWords), words =  'котор')
# создаем матрицу терминов-документов
tdm <- TermDocumentMatrix(news_corpus)
tdm
# создаем матрицу документов-терминов
dtm <- DocumentTermMatrix(news_corpus)
dtm
dtm <- removeSparseTerms(dtm, 0.97)
dtm
dtm <- removeSparseTerms(dtm, 0.97)
dtm
# создаем матрицу документов-терминов
dtm <- DocumentTermMatrix(news_corpus)
dtm
# массив меток – тематики новостей
org_labels<-meta_data[, "category"]
count_data <- length(org_labels)
count_data
# разделим исходные данные на тестовую и обучающую выборки
# число новостей обучающей выборки (70% от исходных данных)
count_train = round(count_data*0.7)
# для применения методов классификации создаем контейнер
container <- create_container(
dtm,
labels = org_labels,
trainSize = 1:count_train,
testSize = (count_train+1):count_data,
virgin = FALSE
)
# проверим репрезентативность выборки
round(prop.table(table(frame[2]))*100, digits = 1)
train_data = frame[1:count_train,]
round(prop.table(table(train_data[2]))*100, digits = 1)
# обучение моделей
# Support vector machine
svm_model <- train_model(container, "SVM")
# Random forest
tree_model <- train_model(container, "TREE")
# Maximal entropy
maxent_model <- train_model(container, "MAXENT")
# запускаем классификацию
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
# создаем общую матрицу меток
labels_out <- data.frame(
correct_label = org_labels[(count_train+1):count_data],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
# SVM performance
table(labels_out[,1] == labels_out[,2])
# Random forest performance
table(labels_out[,1] == labels_out[,3])
# Maximum entropy performance
table(labels_out[,1] == labels_out[,4])
