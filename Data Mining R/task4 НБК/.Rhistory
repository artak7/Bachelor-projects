# DateTime = str_replace_all(DateTime, '/', '-')
# DateTime = str_replace(DateTime, ':', ' ')
# DateTime
# Str = strsplit(DateTime, ' ')
# Time = Str[[1]][2]
# Time
# hour = substring(Time, 1, 2)
# hour
Str = toString(A[i, 6])
# Str
Str = strsplit(Str, ' ')
# Str
Querytype = Str[[1]][1]
Query = Str[[1]][2]
link = str_extract_all(Query, boundary("word"), simplify = TRUE)
# link
id_book = 0
if (length(link) > 2 && link[,2] == "id_book")
{
id_book = link[,3]
id_books = append(id_books, id_book)
}
Query = link[,1]
# Status = A[i, 7]
# bytes = A[i, 8]
ID = A[i, 9]
ID = substring(ID, 3)
# head(IP)
# head(DateTime)
# head(Querytype)
# head(Query)
# head(id_book)
# head(Status)
# head(bytes)
# head(ID)
#
IP = toString(IP)
clients = append(clients, ID)
# clientsph[] =
# values = paste0('"',IP,'",', '"', DateTime,'",', '"', Query, '",',id_book, ',', ID)
# sql <- paste("INSERT INTO tabler VALUES (",values,")" )
# sql
# sqlsave(connection, sql)
# dbSendQuery(connection, sql)
}
for (i in 1: 5000)
{
IP = toString(A[i, 1])
# IP
# DateTime = toString(A[i, 4])
# DateTime = substring(DateTime, 2)
# DateTime
# # DateTime = parse_date_time(DateTime , 'd b Y HMS!')
# # DateTime
# DateTime = str_replace(DateTime, 'Mar', '3')
# DateTime = str_replace_all(DateTime, '/', '-')
# DateTime = str_replace(DateTime, ':', ' ')
# DateTime
# Str = strsplit(DateTime, ' ')
# Time = Str[[1]][2]
# Time
# hour = substring(Time, 1, 2)
# hour
Str = toString(A[i, 6])
# Str
Str = strsplit(Str, ' ')
# Str
Querytype = Str[[1]][1]
Query = Str[[1]][2]
link = str_extract_all(Query, boundary("word"), simplify = TRUE)
# link
id_book = 0
if (length(link) > 2 && link[,2] == "id_book")
{
id_book = link[,3]
id_books = append(id_books, id_book)
}
Query = link[,1]
# Status = A[i, 7]
# bytes = A[i, 8]
ID = A[i, 9]
ID = substring(ID, 3)
# head(IP)
# head(DateTime)
# head(Querytype)
# head(Query)
# head(id_book)
# head(Status)
# head(bytes)
# head(ID)
#
IP = toString(IP)
clients = append(clients, ID)
# clientsph[] =
# values = paste0('"',IP,'",', '"', DateTime,'",', '"', Query, '",',id_book, ',', ID)
# sql <- paste("INSERT INTO tabler VALUES (",values,")" )
# sql
# sqlsave(connection, sql)
# dbSendQuery(connection, sql)
}
setwd("C://Users/pc/Documents/Магистратура/Анализ интернет-данных")
getwd()
library(stringr)
# library(RODBC)
library(RPostgreSQL)
library(lubridate)
library(arules)
library(datasets)
library(stringi)
# m = odbcConnect("db",uid="16393",pwd="123456")
# m = odbcConnect("tabler",uid="16394",pwd="123456")
connection = dbConnect(PostgreSQL(), host='localhost', port="5432", dbname = "db", user = "postgres", password = "123456")
Documents
setwd("C://Users/pc/Documents/Магистратура/Анализ интернет-данных")
getwd()
library(stringr)
# library(RODBC)
library(RPostgreSQL)
library(lubridate)
library(arules)
library(datasets)
library(stringi)
A = read.table("access.log",header=FALSE,sep=" ")
clients = c()
clientsph = c()
id_books = c()
head(A)
for (i in 1: 5000)
{
IP = toString(A[i, 1])
# IP
# DateTime = toString(A[i, 4])
# DateTime = substring(DateTime, 2)
# DateTime
# # DateTime = parse_date_time(DateTime , 'd b Y HMS!')
# # DateTime
# DateTime = str_replace(DateTime, 'Mar', '3')
# DateTime = str_replace_all(DateTime, '/', '-')
# DateTime = str_replace(DateTime, ':', ' ')
# DateTime
# Str = strsplit(DateTime, ' ')
# Time = Str[[1]][2]
# Time
# hour = substring(Time, 1, 2)
# hour
Str = toString(A[i, 6])
# Str
Str = strsplit(Str, ' ')
# Str
Querytype = Str[[1]][1]
Query = Str[[1]][2]
link = str_extract_all(Query, boundary("word"), simplify = TRUE)
# link
id_book = 0
if (length(link) > 2 && link[,2] == "id_book")
{
id_book = link[,3]
id_books = append(id_books, id_book)
}
Query = link[,1]
# Status = A[i, 7]
# bytes = A[i, 8]
ID = A[i, 9]
ID = substring(ID, 3)
# head(IP)
# head(DateTime)
# head(Querytype)
# head(Query)
# head(id_book)
# head(Status)
# head(bytes)
# head(ID)
#
IP = toString(IP)
clients = append(clients, ID)
# clientsph[] =
# values = paste0('"',IP,'",', '"', DateTime,'",', '"', Query, '",',id_book, ',', ID)
# sql <- paste("INSERT INTO tabler VALUES (",values,")" )
# sql
# sqlsave(connection, sql)
# dbSendQuery(connection, sql)
}
for (i in 1: 50000)
{
IP = toString(A[i, 1])
# IP
# DateTime = toString(A[i, 4])
# DateTime = substring(DateTime, 2)
# DateTime
# # DateTime = parse_date_time(DateTime , 'd b Y HMS!')
# # DateTime
# DateTime = str_replace(DateTime, 'Mar', '3')
# DateTime = str_replace_all(DateTime, '/', '-')
# DateTime = str_replace(DateTime, ':', ' ')
# DateTime
# Str = strsplit(DateTime, ' ')
# Time = Str[[1]][2]
# Time
# hour = substring(Time, 1, 2)
# hour
Str = toString(A[i, 6])
# Str
Str = strsplit(Str, ' ')
# Str
Querytype = Str[[1]][1]
Query = Str[[1]][2]
link = str_extract_all(Query, boundary("word"), simplify = TRUE)
# link
id_book = 0
if (length(link) > 2 && link[,2] == "id_book")
{
id_book = link[,3]
id_books = append(id_books, id_book)
}
Query = link[,1]
# Status = A[i, 7]
# bytes = A[i, 8]
ID = A[i, 9]
ID = substring(ID, 3)
# head(IP)
# head(DateTime)
# head(Querytype)
# head(Query)
# head(id_book)
# head(Status)
# head(bytes)
# head(ID)
#
IP = toString(IP)
clients = append(clients, ID)
# clientsph[] =
# values = paste0('"',IP,'",', '"', DateTime,'",', '"', Query, '",',id_book, ',', ID)
# sql <- paste("INSERT INTO tabler VALUES (",values,")" )
# sql
# sqlsave(connection, sql)
# dbSendQuery(connection, sql)
}
for (i in 1: 50000)
{
IP = toString(A[i, 1])
# IP
# DateTime = toString(A[i, 4])
# DateTime = substring(DateTime, 2)
# DateTime
# # DateTime = parse_date_time(DateTime , 'd b Y HMS!')
# # DateTime
# DateTime = str_replace(DateTime, 'Mar', '3')
# DateTime = str_replace_all(DateTime, '/', '-')
# DateTime = str_replace(DateTime, ':', ' ')
# DateTime
# Str = strsplit(DateTime, ' ')
# Time = Str[[1]][2]
# Time
# hour = substring(Time, 1, 2)
# hour
Str = toString(A[i, 6])
# Str
Str = strsplit(Str, ' ')
# Str
# Querytype = Str[[1]][1]
Query = Str[[1]][2]
link = str_extract_all(Query, boundary("word"), simplify = TRUE)
# link
id_book = 0
if (length(link) > 2 && link[,2] == "id_book")
{
id_book = link[,3]
id_books = append(id_books, id_book)
}
Query = link[,1]
# Status = A[i, 7]
# bytes = A[i, 8]
ID = A[i, 9]
ID = substring(ID, 3)
print(ID)
# head(IP)
# head(DateTime)
# head(Querytype)
# head(Query)
# head(id_book)
# head(Status)
# head(bytes)
# head(ID)
#
IP = toString(IP)
clients = append(clients, ID)
# clientsph[] =
# values = paste0('"',IP,'",', '"', DateTime,'",', '"', Query, '",',id_book, ',', ID)
# sql <- paste("INSERT INTO tabler VALUES (",values,")" )
# sql
# sqlsave(connection, sql)
# dbSendQuery(connection, sql)
print(IP)
}
uniqclients = unique(clients)
uniqclients
lcl = length(clients)
mclients = lcl / (31*24)
mclients
#task3
length(id_books)
uniqclients = order(uniqclients)
uniqclients
length(uniqclients)
#task3
length(id_books)
id_books
id_books = sort(id_books)
id_books
setwd("C://Users/pc/Documents/Магистратура/2 семестр/R/task4 НБК")
getwd()
# Прочитаем файл с данными в переменную flagdata
flagdata <- read.csv("flagData.txt", head = FALSE, sep=",") #, sep="\t"
# Формула нормализации данных
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
# Нормализуем данные
# исключим не численные факторы и переменную отклика
flag <- as.data.frame(lapply(flagdata[c(2:6, 8:17, 19:28)], normalize))
dims = dim(flag) # n * m
n = dims[1] # объём выборки # 194
m = dims[2] # количество факторов # 25
# добавим перменную отклика в конец для удобства
flag[m+1] = flagdata[7]
flag
# Сохраним объём всей выборки и объём обучающей выборки в переменных
n_data = n
n_train = 110
# Подсчитаем частоты встречаемости классов в исходной выборке
# (выразим эти частоты в %, округлив результат до десятых)
round(prop.table(table(flag[m+1]))*100, digits = 1) # априорные вероятности гипотез (классов) (p(Hk))
# Чтобы обеспечить репрезентативность выборки, перемешаем её
set.seed(12345)
flagdata_mixed=flag[order(runif(n_data)),]
# Выберем обучающую выборку
train_data = flagdata_mixed[1:n_train,]
# Сохраним номера классов для строк обучающей выборки
train_data_labels = train_data[,m+1]
# формирование меток соответствующих уровней значения переменной отклика
train_data_labels<-factor(train_data_labels)
# Подсчитаем частоту присутствия каждого класса в обучающей выборке
# и сравним с соответствующими частотами в исходной выборке
round(prop.table(table(train_data[m+1]))*100, digits = 1)
# Оставшуюся часть "перемешанной выборки" будем использовать как тестовую выборку
test_data = flagdata_mixed[(n_train+1):n_data, ]
test_data_labels = test_data[,m+1]
# Подключим пакет rpart
library("rpart")
# потсроим дерево
my_tree = rpart(Type ~ 2+3+4+5+6+8+9+10+11+12+13+14+15+16+17+19+20+21+22+23+24+25+26+27+28, method="class",data=train_data)
# Добавим названия столбцов
colnames(flagdata)<- c('a2','a3','a4','a5','a6','a8','a9','a10','a11','a12','a13','a14','a15','a16','a17','a19','a20','a21','a22','a23','a24','a25','a26','a27','a28','Type')
# потсроим дерево
my_tree = rpart(Type ~ a2+a3+a4+a5+a6+a8+a9+a10+a11+a12+a13+a14+a15+a16+a17+a19+a20+a21+a22+a23+a24+a25+a26+a27+a28, method="class",data=train_data)
# Добавим названия столбцов
colnames(flag)<- c('a2','a3','a4','a5','a6','a8','a9','a10','a11','a12','a13','a14','a15','a16','a17','a19','a20','a21','a22','a23','a24','a25','a26','a27','a28','Type')
# потсроим дерево
my_tree = rpart(Type ~ a2+a3+a4+a5+a6+a8+a9+a10+a11+a12+a13+a14+a15+a16+a17+a19+a20+a21+a22+a23+a24+a25+a26+a27+a28, method="class",data=train_data)
# потсроим дерево
my_tree <- rpart(Type ~ a2+a3+a4+a5+a6+a8+a9+a10+a11+a12+a13+a14+a15+a16+a17+a19+a20+a21+a22+a23+a24+a25+a26+a27+a28, method="class",data=train_data)
# потсроим дерево
my_tree <- rpart('Type' ~ a2+a3+a4+a5+a6+a8+a9+a10+a11+a12+a13+a14+a15+a16+a17+a19+a20+a21+a22+a23+a24+a25+a26+a27+a28, method="class",data=train_data)
flag
setwd("C://Users/pc/Documents/Магистратура/2 семестр/R/task4 НБК")
getwd()
# Прочитаем файл с данными в переменную flagdata
flagdata <- read.csv("flagData.txt", head = FALSE, sep=",") #, sep="\t"
# str(flagdata) # структура данных
flagdata
# Формула нормализации данных
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
# Нормализуем данные
# исключим не численные факторы и переменную отклика
flag <- as.data.frame(lapply(flagdata[c(2:6, 8:17, 19:28)], normalize))
dims = dim(flag) # n * m
n = dims[1] # объём выборки # 194
m = dims[2] # количество факторов # 25
# добавим перменную отклика в конец для удобства
flag[m+1] = flagdata[7]
flag
# Сохраним объём всей выборки и объём обучающей выборки в переменных
n_data = n
n_train = 110
# Подсчитаем частоты встречаемости классов в исходной выборке
# (выразим эти частоты в %, округлив результат до десятых)
round(prop.table(table(flag[m+1]))*100, digits = 1) # априорные вероятности гипотез (классов) (p(Hk))
# Чтобы обеспечить репрезентативность выборки, перемешаем её
set.seed(12345)
flagdata_mixed=flag[order(runif(n_data)),]
# Выберем обучающую выборку
train_data = flagdata_mixed[1:n_train,]
# Сохраним номера классов для строк обучающей выборки
train_data_labels = train_data[,m+1]
# формирование меток соответствующих уровней значения переменной отклика
train_data_labels<-factor(train_data_labels)
# Подсчитаем частоту присутствия каждого класса в обучающей выборке
# и сравним с соответствующими частотами в исходной выборке
round(prop.table(table(train_data[m+1]))*100, digits = 1)
# Оставшуюся часть "перемешанной выборки" будем использовать как тестовую выборку
test_data = flagdata_mixed[(n_train+1):n_data, ]
test_data_labels = test_data[,m+1]
# Подключим пакет rpart
library("rpart")
# потсроим дерево
my_tree <- rpart(v7 ~ v2+v3+v4+v5+v6+v8+v9+v10+v11+v12+v13+v14+v15+v16+v17+v19+v20+v21+v22+v23+v24+v25+v26+v27+v28, method="class",data=train_data)
# потсроим дерево
my_tree <- rpart(v2 ~ v3+v4+v5+v6+v8+v9+v10+v11+v12+v13+v14+v15+v16+v17+v19+v20+v21+v22+v23+v24+v25+v26+v27+v28, method="class",data=train_data)
# потсроим дерево
my_tree <- rpart(V7 ~ V2+V3+V4+V5+V6+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17+V19+V20+V21+V22+V23+V24+V25+V26+V27+V28, method="class",data=train_data)
# Построим график дерева
plot(my_tree)
text(my_tree, cex = 1.2,
all = FALSE, use.n = FALSE)
flag
# Построим график дерева
plot(my_tree)
text(my_tree, cex = 1.2, all = FALSE, use.n = FALSE)
# summary(my_tree)
# Можем распечатать совокупность правил дерева,
# но они довольно громоздки
# Протестируем построенное дерево
# Теперь, когда дерево уже построено,
# номера классов (m+1-й столбец) нужно удалить
test_data <- test_data[-(m+1)]
# summary(my_tree)
# Можем распечатать совокупность правил дерева,
# но они довольно громоздки
# Протестируем построенное дерево
# Теперь, когда дерево уже построено,
# номера классов (m+1-й столбец) нужно удалить
test_data <- test_data[-(m+1)]
# "Спрогнозируем" значение переменной отклика
# для тестовых данных
flag_pred <- predict(my_tree,
test_data,
type="class")
# Для оценки качества прогноза
# подключим библиотеку gmodels
library("gmodels")
# Построим кросс-валидационную таблицу:
CrossTable(x = test_data_labels, y = flag_pred, prop.chisq=FALSE)
#очищаем рабочее пространство
rm (list=ls())
setwd("C://Users/pc/Documents/Магистратура/2 семестр/R/task4 НБК")
getwd()
#очищаем рабочее пространство
rm (list=ls())
# Прочитаем файл с данными в переменную flagdata
flagdata <- read.csv("flagData.txt", head = FALSE, sep=",") #, sep="\t"
# str(flagdata) # структура данных
flagdata
# Формула нормализации данных
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
# Нормализуем данные
# исключим не численные факторы и переменную отклика
flag <- as.data.frame(lapply(flagdata[c(2:6, 8:17, 19:28)], normalize))
dims = dim(flag) # n * m
n = dims[1] # объём выборки # 194
m = dims[2] # количество факторов # 25
# добавим перменную отклика в конец для удобства
flag[m+1] = flagdata[7]
flag
# Сохраним объём всей выборки и объём обучающей выборки в переменных
n_data = n
n_train = 110
# Подсчитаем частоты встречаемости классов в исходной выборке
# (выразим эти частоты в %, округлив результат до десятых)
round(prop.table(table(flag[m+1]))*100, digits = 1) # априорные вероятности гипотез (классов) (p(Hk))
# Чтобы обеспечить репрезентативность выборки, перемешаем её
set.seed(12345)
flagdata_mixed=flag[order(runif(n_data)),]
# Выберем обучающую выборку
train_data = flagdata_mixed[1:n_train,]
# Сохраним номера классов для строк обучающей выборки
train_data_labels = train_data[,m+1]
# формирование меток соответствующих уровней значения переменной отклика
train_data_labels<-factor(train_data_labels)
# Подсчитаем частоту присутствия каждого класса в обучающей выборке
# и сравним с соответствующими частотами в исходной выборке
round(prop.table(table(train_data[m+1]))*100, digits = 1)
# Оставшуюся часть "перемешанной выборки" будем использовать как тестовую выборку
test_data = flagdata_mixed[(n_train+1):n_data, ]
test_data_labels = test_data[,m+1]
# Подключим пакет rpart
library("rpart")
# построим дерево
my_tree <- rpart(V7 ~ V2+V3+V4+V5+V6+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17+V19+V20+V21+V22+V23+V24+V25+V26+V27+V28, method="class",data=train_data)
# Построим график дерева
plot(my_tree)
text(my_tree, cex = 1.2, all = FALSE, use.n = FALSE)
# summary(my_tree)
# Можем распечатать совокупность правил дерева,
# но они довольно громоздки
# Протестируем построенное дерево
# Теперь, когда дерево уже построено,
# номера классов (m+1-й столбец) нужно удалить
test_data <- test_data[-(m+1)]
# "Спрогнозируем" значение переменной отклика
# для тестовых данных
flag_pred <- predict(my_tree,
test_data,
type="class")
# Для оценки качества прогноза
# подключим библиотеку gmodels
library("gmodels")
# Построим кросс-валидационную таблицу:
CrossTable(x = test_data_labels, y = flag_pred, prop.chisq=FALSE)
# Зададим новые данные
predict <- read.csv("predict.txt", sep=",", head=FALSE)
predict = predict[c(2:6, 8:17, 19:28)]
# Определим значение переменной отклика (номер класса) для новых данных
new_predict <- predict(my_tree, predict, type="class")
new_predict
